/**
 * åšå®¢å›¾ç‰‡å‹ç¼©ä¼˜åŒ–æœåŠ¡
 * ä½¿ç”¨ Sharp ç”Ÿæˆå¤šç§å°ºå¯¸å’Œæ ¼å¼çš„å›¾ç‰‡
 */

import sharp from 'sharp'
import fs from 'fs/promises'
import path from 'path'

// Vercel Serverless Functions åªèƒ½å†™å…¥ /tmp ç›®å½•
const COMPRESSED_DIR =
  process.env.VERCEL || process.env.NODE_ENV === 'production'
    ? '/tmp/blog-images/compressed'
    : path.join(process.cwd(), 'tmp', 'blog-images', 'compressed')

export interface OptimizedImages {
  original: string      // 1200x630 JPEG (85%)
  thumbnail: string     // 600x315 JPEG (80%)
  webp: string         // 1200x630 WebP (80%)
}

/**
 * å‹ç¼©å’Œä¼˜åŒ–åšå®¢å›¾ç‰‡
 * ç”Ÿæˆå¤šç§å°ºå¯¸å’Œæ ¼å¼
 * @param inputPath åŸå§‹å›¾ç‰‡è·¯å¾„
 * @returns å‹ç¼©åçš„å›¾ç‰‡è·¯å¾„
 */
export async function optimizeBlogImage(inputPath: string): Promise<OptimizedImages> {
  try {
    // ç¡®ä¿å‹ç¼©ç›®å½•å­˜åœ¨
    await fs.mkdir(COMPRESSED_DIR, { recursive: true })

    // è·å–åŸºç¡€æ–‡ä»¶å (ä¸å«æ‰©å±•å)
    const basename = path.basename(inputPath, path.extname(inputPath))

    // è¾“å‡ºè·¯å¾„
    const originalPath = path.join(COMPRESSED_DIR, `${basename}-original.jpg`)
    const thumbnailPath = path.join(COMPRESSED_DIR, `${basename}-thumb.jpg`)
    const webpPath = path.join(COMPRESSED_DIR, `${basename}.webp`)

    console.log('ğŸ”§ Optimizing blog image:', inputPath)

    // åŸå›¾å‹ç¼© (1200x630, JPEG 85%)
    console.log('  ğŸ“¸ Generating original (1200x630, JPEG 85%)...')
    await sharp(inputPath)
      .resize(1200, 630, {
        fit: 'cover',
        position: 'center',
      })
      .jpeg({ quality: 85 })
      .toFile(originalPath)

    // ç¼©ç•¥å›¾ (600x315, JPEG 80%)
    console.log('  ğŸ–¼ï¸  Generating thumbnail (600x315, JPEG 80%)...')
    await sharp(inputPath)
      .resize(600, 315, {
        fit: 'cover',
        position: 'center',
      })
      .jpeg({ quality: 80 })
      .toFile(thumbnailPath)

    // WebP æ ¼å¼ (1200x630, 80%)
    console.log('  ğŸŒ Generating WebP (1200x630, 80%)...')
    await sharp(inputPath)
      .resize(1200, 630, {
        fit: 'cover',
        position: 'center',
      })
      .webp({ quality: 80 })
      .toFile(webpPath)

    // è·å–æ–‡ä»¶å¤§å°
    const originalStats = await fs.stat(originalPath)
    const thumbnailStats = await fs.stat(thumbnailPath)
    const webpStats = await fs.stat(webpPath)

    console.log('âœ… Image optimization complete:')
    console.log(`  ğŸ“¦ Original: ${(originalStats.size / 1024).toFixed(2)} KB`)
    console.log(`  ğŸ“¦ Thumbnail: ${(thumbnailStats.size / 1024).toFixed(2)} KB`)
    console.log(`  ğŸ“¦ WebP: ${(webpStats.size / 1024).toFixed(2)} KB`)

    return {
      original: originalPath,
      thumbnail: thumbnailPath,
      webp: webpPath,
    }
  } catch (error) {
    console.error('âŒ Image optimization failed:', error)
    throw error
  }
}

/**
 * æ¸…ç†å‹ç¼©ç›®å½•ä¸­çš„æ—§æ–‡ä»¶
 */
export async function cleanupCompressedImages(): Promise<void> {
  try {
    const files = await fs.readdir(COMPRESSED_DIR)
    const now = Date.now()
    const ONE_HOUR = 60 * 60 * 1000

    for (const file of files) {
      const filePath = path.join(COMPRESSED_DIR, file)
      const stats = await fs.stat(filePath)

      // åˆ é™¤è¶…è¿‡ 1 å°æ—¶çš„å‹ç¼©æ–‡ä»¶
      if (now - stats.mtimeMs > ONE_HOUR) {
        await fs.unlink(filePath)
        console.log(`ğŸ—‘ï¸  Cleaned up old compressed file: ${file}`)
      }
    }
  } catch (error) {
    console.error('âš ï¸  Failed to cleanup compressed images:', error)
  }
}
